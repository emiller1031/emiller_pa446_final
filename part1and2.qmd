---
title: "Parts I & II"
format: pdf
editor: visual
---

```{r}
library(tidyverse)
library(usethis)
library(rvest)
library(gt)
library(tidytext)
library(readr)
library(tokenizers)
```

## Part I: Web Scraping

Go to the website <https://www.scrapethissite.com/pages/simple/> and scrape the data to create
a table with four variables: Country, Capital, Population, and Area. The table will have a total of
250 observations.

```{r}
url <- "https://www.scrapethissite.com/pages/simple/"
html <- read_html(url)

country <- html |>
  html_elements("h3.country-name") |> 
  html_text2() 

capital <- html |>
  html_elements("span.country-capital") |> 
  html_text2() 

population <- html %>%
  html_elements("span.country-population") %>%
  html_text2()

area <- html |>
  html_elements("span.country-area") |> 
  html_text2() 

# Build tibble
country_table <- tibble(country, capital, population, area)
country_table
```

## Part II: Text Analysis

Use the artofwar dataset and conduct a text analysis.

Tokenize the data, compute word counts, remove stop words, and create bar plot showing dataset's top ten used words, flipping the axes.

```{r}
# Tokenize the data
text_data <- read_csv("artofwar.csv")

text_data <- text_data %>%
  rename("origcol" = x) # the column name was giving me trouble, so i renamed it

tokens <- text_data %>%
  unnest_tokens(words, origcol)

# Compute word counts
```
